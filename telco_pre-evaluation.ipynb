{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# ALGORITHMS:\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerID          0\n",
       "gender              0\n",
       "SeniorCitizen       0\n",
       "Partner             0\n",
       "Dependents          0\n",
       "tenure              0\n",
       "PhoneService        0\n",
       "MultipleLines       0\n",
       "InternetService     0\n",
       "OnlineSecurity      0\n",
       "OnlineBackup        0\n",
       "DeviceProtection    0\n",
       "TechSupport         0\n",
       "StreamingTV         0\n",
       "StreamingMovies     0\n",
       "Contract            0\n",
       "PaperlessBilling    0\n",
       "PaymentMethod       0\n",
       "MonthlyCharges      0\n",
       "TotalCharges        0\n",
       "Churn               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX = pd.read_csv(\"Telco-Customer-Churn.csv\")\n",
    "dataX.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerID           0\n",
       "gender               0\n",
       "SeniorCitizen        0\n",
       "Partner              0\n",
       "Dependents           0\n",
       "tenure               0\n",
       "PhoneService         0\n",
       "MultipleLines        0\n",
       "InternetService      0\n",
       "OnlineSecurity       0\n",
       "OnlineBackup         0\n",
       "DeviceProtection     0\n",
       "TechSupport          0\n",
       "StreamingTV          0\n",
       "StreamingMovies      0\n",
       "Contract             0\n",
       "PaperlessBilling     0\n",
       "PaymentMethod        0\n",
       "MonthlyCharges       0\n",
       "TotalCharges        11\n",
       "Churn                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX.replace(\" \", np.nan, inplace=True)\n",
    "dataX.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX[\"Churn\"]=dataX[\"Churn\"].replace({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX[['TotalCharges']]=dataX[[\"TotalCharges\"]].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX[\"TotalCharges\"]=dataX[\"TotalCharges\"].fillna(dataX[\"TotalCharges\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 21 columns):\n",
      "customerID          7043 non-null object\n",
      "gender              7043 non-null object\n",
      "SeniorCitizen       7043 non-null int64\n",
      "Partner             7043 non-null object\n",
      "Dependents          7043 non-null object\n",
      "tenure              7043 non-null int64\n",
      "PhoneService        7043 non-null object\n",
      "MultipleLines       7043 non-null object\n",
      "InternetService     7043 non-null object\n",
      "OnlineSecurity      7043 non-null object\n",
      "OnlineBackup        7043 non-null object\n",
      "DeviceProtection    7043 non-null object\n",
      "TechSupport         7043 non-null object\n",
      "StreamingTV         7043 non-null object\n",
      "StreamingMovies     7043 non-null object\n",
      "Contract            7043 non-null object\n",
      "PaperlessBilling    7043 non-null object\n",
      "PaymentMethod       7043 non-null object\n",
      "MonthlyCharges      7043 non-null float64\n",
      "TotalCharges        7043 non-null float64\n",
      "Churn               7043 non-null int64\n",
      "dtypes: float64(2), int64(3), object(16)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dataX.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Separate target from predictors\n",
    "y = dataX.Churn\n",
    "X = dataX.drop(['Churn'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# Drop columns with missing values (simplest approach)\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns!\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n"
     ]
    }
   ],
   "source": [
    "# Choosing CATEGORICAL variables only!\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = XGBClassifier(n_estimators=500)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONE-HOT ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from One-Hot Encoding:\n",
      "0.21646557842441447\n"
     ]
    }
   ],
   "source": [
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "print(\"Mean Absolute Error from One-Hot Encoding:\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LABEL ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Label Encoding:\n",
      "0.21788502484031227\n"
     ]
    }
   ],
   "source": [
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply label encoder to each column with categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "print(\"Mean Absolute Error from Label Encoding:\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model evaluation using LABEL ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:28:45] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS OF MODEL EVALUATION BASED ON LABEL ENCODER!\n",
      "Mean Absolute Error for XGBRegressor: 0.2741667470075798\n",
      "Mean Absolute Error for XGBClassifier: 0.2036905606813343\n",
      "Mean Absolute Error for RandomForestRegressor: 0.2735344215755855\n",
      "Mean Absolute Error for RandomForestClassifier: 0.23562810503903478\n",
      "Mean Absolute Error for GradientBoostingRegressor: 0.27421928626229963\n",
      "Mean Absolute Error for GradientBoostingClassifier: 0.20723917672107878\n",
      "Mean Absolute Error for AdaBoostRegressor: 0.31213342723355486\n",
      "Mean Absolute Error for AdaBoostClassifier: 0.21433640880056778\n",
      "Mean Absolute Error for BaggingRegressor: 0.2749940856399337\n",
      "Mean Absolute Error for BaggingClassifier: 0.23562810503903478\n",
      "Mean Absolute Error for ExtraTreesRegressor: 0.2831795599716111\n",
      "Mean Absolute Error for ExtraTreesClassifier: 0.2313697657913414\n",
      "Mean Absolute Error for Support Vector Regression: 0.34992531989118286\n",
      "Mean Absolute Error for Support Vector Classification: 0.23491838183108588\n",
      "Mean Absolute Error for Linear Support Vector Regression: 0.6209946027954137\n",
      "Mean Absolute Error for Linear Support Vector Classification: 0.5230660042583393\n",
      "Mean Absolute Error for Nu-Support Vector Regression: 0.31441923163776303\n",
      "Mean Absolute Error for Nu-Support Vector Classification: 0.24485450674237047\n",
      "Mean Absolute Error for (Neural Net):Multi-layer Perceptron Regressor: 0.3131352976026875\n",
      "Mean Absolute Error for (Neural Net):Multi-layer Perceptron Classifier: 0.22853087295954577\n",
      "Mean Absolute Error for KNeighborsRegressor: 0.28857345635202275\n",
      "Mean Absolute Error for KNeighborsClassifier: 0.23917672107877927\n",
      "Mean Absolute Error for GaussianNB: 0.262597586941093\n",
      "Mean Absolute Error for MultinomialNB: 0.36266855926188785\n",
      "Mean Absolute Error for LinearRegression: 0.3066088649534698\n",
      "Mean Absolute Error for LogisticRegression: 0.20652945351312987\n"
     ]
    }
   ],
   "source": [
    "my_model1 = XGBRegressor()\n",
    "my_model1.fit(label_X_train, y_train)\n",
    "\n",
    "my_model2 = XGBClassifier()\n",
    "my_model2.fit(label_X_train, y_train)\n",
    "\n",
    "my_model3 = RandomForestRegressor()\n",
    "my_model3.fit(label_X_train, y_train)\n",
    "\n",
    "my_model4 = RandomForestClassifier()\n",
    "my_model4.fit(label_X_train, y_train)\n",
    "\n",
    "my_model5 = GradientBoostingRegressor()\n",
    "my_model5.fit(label_X_train, y_train)\n",
    "\n",
    "my_model6 = GradientBoostingClassifier()\n",
    "my_model6.fit(label_X_train, y_train)\n",
    "\n",
    "my_model7 = AdaBoostRegressor()\n",
    "my_model7.fit(label_X_train, y_train)\n",
    "\n",
    "my_model8 = AdaBoostClassifier()\n",
    "my_model8.fit(label_X_train, y_train)\n",
    "\n",
    "my_model9 = BaggingRegressor()\n",
    "my_model9.fit(label_X_train, y_train)\n",
    "\n",
    "my_model10 = BaggingClassifier()\n",
    "my_model10.fit(label_X_train, y_train)\n",
    "\n",
    "my_model11 = ExtraTreesRegressor()\n",
    "my_model11.fit(label_X_train, y_train)\n",
    "\n",
    "my_model12 = ExtraTreesClassifier()\n",
    "my_model12.fit(label_X_train, y_train)\n",
    "\n",
    "my_model13 = SVR()\n",
    "my_model13.fit(label_X_train, y_train)\n",
    "\n",
    "my_model14 = SVC()\n",
    "my_model14.fit(label_X_train, y_train)\n",
    "\n",
    "my_model15 = LinearSVR()\n",
    "my_model15.fit(label_X_train, y_train)\n",
    "\n",
    "my_model16 = LinearSVC()\n",
    "my_model16.fit(label_X_train, y_train)\n",
    "\n",
    "my_model17 = NuSVR()\n",
    "my_model17.fit(label_X_train, y_train)\n",
    "\n",
    "my_model18 = NuSVC()\n",
    "my_model18.fit(label_X_train, y_train)\n",
    "\n",
    "my_model19 = MLPRegressor()\n",
    "my_model19.fit(label_X_train, y_train)\n",
    "\n",
    "my_model20 = MLPClassifier()\n",
    "my_model20.fit(label_X_train, y_train)\n",
    "\n",
    "my_model21 = KNeighborsRegressor()\n",
    "my_model21.fit(label_X_train, y_train)\n",
    "\n",
    "my_model22 = KNeighborsClassifier()\n",
    "my_model22.fit(label_X_train, y_train)\n",
    "\n",
    "my_model23 = GaussianNB()\n",
    "my_model23.fit(label_X_train, y_train)\n",
    "\n",
    "my_model24 = MultinomialNB()\n",
    "my_model24.fit(label_X_train, y_train)\n",
    "\n",
    "my_model25 = LinearRegression()\n",
    "my_model25.fit(label_X_train, y_train)\n",
    "\n",
    "my_model26 = LogisticRegression()\n",
    "my_model26.fit(label_X_train, y_train)\n",
    "\n",
    "\n",
    "predictions1 = my_model1.predict(label_X_valid)\n",
    "predictions2 = my_model2.predict(label_X_valid)\n",
    "predictions3 = my_model3.predict(label_X_valid)\n",
    "predictions4 = my_model4.predict(label_X_valid)\n",
    "predictions5 = my_model5.predict(label_X_valid)\n",
    "predictions6 = my_model6.predict(label_X_valid)\n",
    "predictions7 = my_model7.predict(label_X_valid)\n",
    "predictions8 = my_model8.predict(label_X_valid)\n",
    "predictions9 = my_model9.predict(label_X_valid)\n",
    "predictions10 = my_model10.predict(label_X_valid)\n",
    "predictions11 = my_model11.predict(label_X_valid)\n",
    "predictions12 = my_model12.predict(label_X_valid)\n",
    "predictions13 = my_model13.predict(label_X_valid)\n",
    "predictions14 = my_model14.predict(label_X_valid)\n",
    "predictions15 = my_model15.predict(label_X_valid)\n",
    "predictions16 = my_model16.predict(label_X_valid)\n",
    "predictions17 = my_model17.predict(label_X_valid)\n",
    "predictions18 = my_model18.predict(label_X_valid)\n",
    "predictions19 = my_model19.predict(label_X_valid)\n",
    "predictions20 = my_model20.predict(label_X_valid)\n",
    "predictions21 = my_model21.predict(label_X_valid)\n",
    "predictions22 = my_model22.predict(label_X_valid)\n",
    "predictions23 = my_model23.predict(label_X_valid)\n",
    "predictions24 = my_model24.predict(label_X_valid)\n",
    "predictions25 = my_model25.predict(label_X_valid)\n",
    "predictions26 = my_model26.predict(label_X_valid)\n",
    "\n",
    "\n",
    "print(\"RESULTS OF MODEL EVALUATION BASED ON LABEL ENCODER!\")\n",
    "print(\"Mean Absolute Error for XGBRegressor: \" + str(mean_absolute_error(predictions1, y_valid)))\n",
    "print(\"Mean Absolute Error for XGBClassifier: \" + str(mean_absolute_error(predictions2, y_valid)))\n",
    "print(\"Mean Absolute Error for RandomForestRegressor: \" + str(mean_absolute_error(predictions3, y_valid)))\n",
    "print(\"Mean Absolute Error for RandomForestClassifier: \" + str(mean_absolute_error(predictions4, y_valid)))\n",
    "print(\"Mean Absolute Error for GradientBoostingRegressor: \" + str(mean_absolute_error(predictions5, y_valid)))\n",
    "print(\"Mean Absolute Error for GradientBoostingClassifier: \" + str(mean_absolute_error(predictions6, y_valid)))\n",
    "print(\"Mean Absolute Error for AdaBoostRegressor: \" + str(mean_absolute_error(predictions7, y_valid)))\n",
    "print(\"Mean Absolute Error for AdaBoostClassifier: \" + str(mean_absolute_error(predictions8, y_valid)))\n",
    "print(\"Mean Absolute Error for BaggingRegressor: \" + str(mean_absolute_error(predictions9, y_valid)))\n",
    "print(\"Mean Absolute Error for BaggingClassifier: \" + str(mean_absolute_error(predictions10, y_valid)))\n",
    "print(\"Mean Absolute Error for ExtraTreesRegressor: \" + str(mean_absolute_error(predictions11, y_valid)))\n",
    "print(\"Mean Absolute Error for ExtraTreesClassifier: \" + str(mean_absolute_error(predictions12, y_valid)))\n",
    "print(\"Mean Absolute Error for Support Vector Regression: \" + str(mean_absolute_error(predictions13, y_valid)))\n",
    "print(\"Mean Absolute Error for Support Vector Classification: \" + str(mean_absolute_error(predictions14, y_valid)))\n",
    "print(\"Mean Absolute Error for Linear Support Vector Regression: \" + str(mean_absolute_error(predictions15, y_valid)))\n",
    "print(\"Mean Absolute Error for Linear Support Vector Classification: \" + str(mean_absolute_error(predictions16, y_valid)))\n",
    "print(\"Mean Absolute Error for Nu-Support Vector Regression: \" + str(mean_absolute_error(predictions17, y_valid)))\n",
    "print(\"Mean Absolute Error for Nu-Support Vector Classification: \" + str(mean_absolute_error(predictions18, y_valid)))\n",
    "print(\"Mean Absolute Error for (Neural Net):Multi-layer Perceptron Regressor: \" + str(mean_absolute_error(predictions19, y_valid)))\n",
    "print(\"Mean Absolute Error for (Neural Net):Multi-layer Perceptron Classifier: \" + str(mean_absolute_error(predictions20, y_valid)))\n",
    "print(\"Mean Absolute Error for KNeighborsRegressor: \" + str(mean_absolute_error(predictions21, y_valid)))\n",
    "print(\"Mean Absolute Error for KNeighborsClassifier: \" + str(mean_absolute_error(predictions22, y_valid)))\n",
    "print(\"Mean Absolute Error for GaussianNB: \" + str(mean_absolute_error(predictions23, y_valid)))\n",
    "print(\"Mean Absolute Error for MultinomialNB: \" + str(mean_absolute_error(predictions24, y_valid)))\n",
    "print(\"Mean Absolute Error for LinearRegression: \" + str(mean_absolute_error(predictions25, y_valid)))\n",
    "print(\"Mean Absolute Error for LogisticRegression: \" + str(mean_absolute_error(predictions26, y_valid)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation using ONE-HOT ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:29:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\X\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS OF MODEL EVALUATION BASED ON ONE-HOT ENCODER!\n",
      "Mean Absolute Error for XGBRegressor: 0.2746184348129397\n",
      "Mean Absolute Error for XGBClassifier: 0.21291696238466998\n",
      "Mean Absolute Error for RandomForestRegressor: 0.266721923687857\n",
      "Mean Absolute Error for RandomForestClassifier: 0.2207239176721079\n",
      "Mean Absolute Error for GradientBoostingRegressor: 0.2740561621075471\n",
      "Mean Absolute Error for GradientBoostingClassifier: 0.2150461320085167\n",
      "Mean Absolute Error for AdaBoostRegressor: 0.3254201732855699\n",
      "Mean Absolute Error for AdaBoostClassifier: 0.20865862313697658\n",
      "Mean Absolute Error for BaggingRegressor: 0.27622917300348104\n",
      "Mean Absolute Error for BaggingClassifier: 0.22569198012775019\n",
      "Mean Absolute Error for ExtraTreesRegressor: 0.2727111426543648\n",
      "Mean Absolute Error for ExtraTreesClassifier: 0.2512420156139106\n",
      "Mean Absolute Error for Support Vector Regression: 0.3321192616471008\n",
      "Mean Absolute Error for Support Vector Classification: 0.23562810503903478\n",
      "Mean Absolute Error for Linear Support Vector Regression: 0.6117196712341675\n",
      "Mean Absolute Error for Linear Support Vector Classification: 0.22995031937544358\n",
      "Mean Absolute Error for Nu-Support Vector Regression: 0.2928343180540941\n",
      "Mean Absolute Error for Nu-Support Vector Classification: 0.24485450674237047\n",
      "Mean Absolute Error for (Neural Net):Multi-layer Perceptron Regressor: 0.2871504101148539\n",
      "Mean Absolute Error for (Neural Net):Multi-layer Perceptron Classifier: 0.21433640880056778\n",
      "Mean Absolute Error for KNeighborsRegressor: 0.2905606813342796\n",
      "Mean Absolute Error for KNeighborsClassifier: 0.24343506032647266\n",
      "Mean Absolute Error for GaussianNB: 0.30872959545777146\n",
      "Mean Absolute Error for MultinomialNB: 0.3371185237757275\n",
      "Mean Absolute Error for LinearRegression: 0.30104244487474724\n",
      "Mean Absolute Error for LogisticRegression: 0.20014194464158977\n"
     ]
    }
   ],
   "source": [
    "my_model1 = XGBRegressor()\n",
    "my_model1.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model2 = XGBClassifier()\n",
    "my_model2.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model3 = RandomForestRegressor()\n",
    "my_model3.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model4 = RandomForestClassifier()\n",
    "my_model4.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model5 = GradientBoostingRegressor()\n",
    "my_model5.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model6 = GradientBoostingClassifier()\n",
    "my_model6.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model7 = AdaBoostRegressor()\n",
    "my_model7.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model8 = AdaBoostClassifier()\n",
    "my_model8.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model9 = BaggingRegressor()\n",
    "my_model9.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model10 = BaggingClassifier()\n",
    "my_model10.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model11 = ExtraTreesRegressor()\n",
    "my_model11.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model12 = ExtraTreesClassifier()\n",
    "my_model12.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model13 = SVR()\n",
    "my_model13.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model14 = SVC()\n",
    "my_model14.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model15 = LinearSVR()\n",
    "my_model15.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model16 = LinearSVC()\n",
    "my_model16.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model17 = NuSVR()\n",
    "my_model17.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model18 = NuSVC()\n",
    "my_model18.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model19 = MLPRegressor()\n",
    "my_model19.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model20 = MLPClassifier()\n",
    "my_model20.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model21 = KNeighborsRegressor()\n",
    "my_model21.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model22 = KNeighborsClassifier()\n",
    "my_model22.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model23 = GaussianNB()\n",
    "my_model23.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model24 = MultinomialNB()\n",
    "my_model24.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model25 = LinearRegression()\n",
    "my_model25.fit(OH_X_train, y_train)\n",
    "\n",
    "my_model26 = LogisticRegression()\n",
    "my_model26.fit(OH_X_train, y_train)\n",
    "\n",
    "\n",
    "predictions1 = my_model1.predict(OH_X_valid)\n",
    "predictions2 = my_model2.predict(OH_X_valid)\n",
    "predictions3 = my_model3.predict(OH_X_valid)\n",
    "predictions4 = my_model4.predict(OH_X_valid)\n",
    "predictions5 = my_model5.predict(OH_X_valid)\n",
    "predictions6 = my_model6.predict(OH_X_valid)\n",
    "predictions7 = my_model7.predict(OH_X_valid)\n",
    "predictions8 = my_model8.predict(OH_X_valid)\n",
    "predictions9 = my_model9.predict(OH_X_valid)\n",
    "predictions10 = my_model10.predict(OH_X_valid)\n",
    "predictions11 = my_model11.predict(OH_X_valid)\n",
    "predictions12 = my_model12.predict(OH_X_valid)\n",
    "predictions13 = my_model13.predict(OH_X_valid)\n",
    "predictions14 = my_model14.predict(OH_X_valid)\n",
    "predictions15 = my_model15.predict(OH_X_valid)\n",
    "predictions16 = my_model16.predict(OH_X_valid)\n",
    "predictions17 = my_model17.predict(OH_X_valid)\n",
    "predictions18 = my_model18.predict(OH_X_valid)\n",
    "predictions19 = my_model19.predict(OH_X_valid)\n",
    "predictions20 = my_model20.predict(OH_X_valid)\n",
    "predictions21 = my_model21.predict(OH_X_valid)\n",
    "predictions22 = my_model22.predict(OH_X_valid)\n",
    "predictions23 = my_model23.predict(OH_X_valid)\n",
    "predictions24 = my_model24.predict(OH_X_valid)\n",
    "predictions25 = my_model25.predict(OH_X_valid)\n",
    "predictions26 = my_model26.predict(OH_X_valid)\n",
    "\n",
    "\n",
    "print(\"RESULTS OF MODEL EVALUATION BASED ON ONE-HOT ENCODER!\")\n",
    "print(\"Mean Absolute Error for XGBRegressor: \" + str(mean_absolute_error(predictions1, y_valid)))\n",
    "print(\"Mean Absolute Error for XGBClassifier: \" + str(mean_absolute_error(predictions2, y_valid)))\n",
    "print(\"Mean Absolute Error for RandomForestRegressor: \" + str(mean_absolute_error(predictions3, y_valid)))\n",
    "print(\"Mean Absolute Error for RandomForestClassifier: \" + str(mean_absolute_error(predictions4, y_valid)))\n",
    "print(\"Mean Absolute Error for GradientBoostingRegressor: \" + str(mean_absolute_error(predictions5, y_valid)))\n",
    "print(\"Mean Absolute Error for GradientBoostingClassifier: \" + str(mean_absolute_error(predictions6, y_valid)))\n",
    "print(\"Mean Absolute Error for AdaBoostRegressor: \" + str(mean_absolute_error(predictions7, y_valid)))\n",
    "print(\"Mean Absolute Error for AdaBoostClassifier: \" + str(mean_absolute_error(predictions8, y_valid)))\n",
    "print(\"Mean Absolute Error for BaggingRegressor: \" + str(mean_absolute_error(predictions9, y_valid)))\n",
    "print(\"Mean Absolute Error for BaggingClassifier: \" + str(mean_absolute_error(predictions10, y_valid)))\n",
    "print(\"Mean Absolute Error for ExtraTreesRegressor: \" + str(mean_absolute_error(predictions11, y_valid)))\n",
    "print(\"Mean Absolute Error for ExtraTreesClassifier: \" + str(mean_absolute_error(predictions12, y_valid)))\n",
    "print(\"Mean Absolute Error for Support Vector Regression: \" + str(mean_absolute_error(predictions13, y_valid)))\n",
    "print(\"Mean Absolute Error for Support Vector Classification: \" + str(mean_absolute_error(predictions14, y_valid)))\n",
    "print(\"Mean Absolute Error for Linear Support Vector Regression: \" + str(mean_absolute_error(predictions15, y_valid)))\n",
    "print(\"Mean Absolute Error for Linear Support Vector Classification: \" + str(mean_absolute_error(predictions16, y_valid)))\n",
    "print(\"Mean Absolute Error for Nu-Support Vector Regression: \" + str(mean_absolute_error(predictions17, y_valid)))\n",
    "print(\"Mean Absolute Error for Nu-Support Vector Classification: \" + str(mean_absolute_error(predictions18, y_valid)))\n",
    "print(\"Mean Absolute Error for (Neural Net):Multi-layer Perceptron Regressor: \" + str(mean_absolute_error(predictions19, y_valid)))\n",
    "print(\"Mean Absolute Error for (Neural Net):Multi-layer Perceptron Classifier: \" + str(mean_absolute_error(predictions20, y_valid)))\n",
    "print(\"Mean Absolute Error for KNeighborsRegressor: \" + str(mean_absolute_error(predictions21, y_valid)))\n",
    "print(\"Mean Absolute Error for KNeighborsClassifier: \" + str(mean_absolute_error(predictions22, y_valid)))\n",
    "print(\"Mean Absolute Error for GaussianNB: \" + str(mean_absolute_error(predictions23, y_valid)))\n",
    "print(\"Mean Absolute Error for MultinomialNB: \" + str(mean_absolute_error(predictions24, y_valid)))\n",
    "print(\"Mean Absolute Error for LinearRegression: \" + str(mean_absolute_error(predictions25, y_valid)))\n",
    "print(\"Mean Absolute Error for LogisticRegression: \" + str(mean_absolute_error(predictions26, y_valid)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
